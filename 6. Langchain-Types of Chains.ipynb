{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a508f309-0246-40d7-a973-7a55f7776a5c",
   "metadata": {},
   "source": [
    "### LLM CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4e1f697-9077-4523-be95-2200ba8c1653",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_31400\\2703165350.py:17: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(llm=llm, prompt=prompt)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Langchain is a framework for developing applications powered by language models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
    "\n",
    "\n",
    "prompt = PromptTemplate.from_template(\"Write a short sentence about {topic}.\")\n",
    "\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "\n",
    "result = chain.invoke({\"topic\": \"langchain\"})\n",
    "print(result[\"text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d9cbf1-6e45-4a75-81f4-b4883cc03407",
   "metadata": {},
   "source": [
    "### RETRIEVAL QA CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cf3985a5-87a7-4fa7-ba58-9e4489e76bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a library for building LLM applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.schema import Document\n",
    "\n",
    "# Documents\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain is a library for building LLM applications.\"),\n",
    "    Document(page_content=\"Gemini is Google’s latest large language model.\")\n",
    "]\n",
    "\n",
    "#Embedding\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# Vector store\n",
    "vectorstore = FAISS.from_documents(docs, embeddings)\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "# RAG chain\n",
    "retriever_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=False\n",
    ")\n",
    "\n",
    "result = retriever_chain.invoke({\"query\": \"What is LangChain?\"})\n",
    "print(result[\"result\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dd50731-cafb-4aee-9edd-8957ef84ed90",
   "metadata": {},
   "source": [
    "### RETRIEVAL QA WITH SOURCES CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "374933fa-3745-4c38-8b37-fe29256a70e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain_google_genai.chat_models._chat_with_retry.<locals>._chat_with_retry in 2.0 seconds as it raised ResourceExhausted: 429 Resource exhausted. Please try again later. Please refer to https://cloud.google.com/vertex-ai/generative-ai/docs/error-code-429 for more details..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ANSWER: LangChain supports RAG pipelines.\n",
      "\n",
      "SOURCES: doc1\n"
     ]
    }
   ],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "from langchain.schema import Document\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain supports RAG pipelines.\",metadata={\"source\": \"doc1\"}),\n",
    "    Document(page_content=\"Agents help connect LLMs to tools.\",metadata={\"source\": \"doc2\"})\n",
    "]\n",
    "\n",
    "db = FAISS.from_documents(docs, embeddings)\n",
    "retriever = db.as_retriever()\n",
    "\n",
    "qa = RetrievalQAWithSourcesChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "result = qa.invoke({\"question\": \"What does LangChain support?\"})\n",
    "\n",
    "print(\"ANSWER:\", result[\"answer\"])\n",
    "print(\"SOURCES:\", result[\"sources\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf1cba92-bee2-4108-aa0b-0c471b754ca9",
   "metadata": {},
   "source": [
    "### SEQUENTIAL CHAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c9617bc6-ed8e-4efc-abde-97ac17ee1290",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I love LangChain and Gemini because they allow me to build incredible applications very quickly.\n",
      "Final Summarized Response (in Tamil): லாங் செயின் மற்றும் ஜெமினி என்னை நம்பமுடியாத பயன்பாடுகளை மிக விரைவாக உருவாக்க அனுமதிப்பதால் எனக்கு மிகவும் பிடிக்கும்.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# 1. Setup (assuming GOOGLE_API_KEY is set in environment)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# --- STEP 1: Translator Chain ---\n",
    "# Input: original_text\n",
    "# Output: translated_text\n",
    "translate_prompt = PromptTemplate.from_template(\"Translate the following text into Tamil: \\n\\n{original_text}\")\n",
    "translator_chain = translate_prompt | llm | parser\n",
    "\n",
    "# --- STEP 2: Summarizer Chain ---\n",
    "# Input: translated_text (which comes from the translator_chain output)\n",
    "# Output: final_summary\n",
    "summarize_prompt = PromptTemplate.from_template(\"You are an expert Tamil summarizer. Summarize the following Tamil text in one sentence: \\n\\n{translated_text}\")\n",
    "summarizer_chain = summarize_prompt | llm | parser\n",
    "\n",
    "# --- Sequential Chain with LCEL ---\n",
    "# The output of 'translator_chain' automatically becomes the input for 'summarizer_chain'\n",
    "# Note the use of the '|' operator.\n",
    "\n",
    " # Define the initial input for the translator chain\n",
    " # Pass the output of the translator_chain to the next prompt's input variable 'translated_text'\n",
    "sequential_chain = {\"original_text\": lambda x: x['text'] } | translator_chain | {  \"translated_text\": lambda x: x} | summarizer_chain\n",
    "#sequential_chain =  translator_chain | summarizer_chain\n",
    "\n",
    "# --- Invoke the Chain ---\n",
    "input_text = \"I love LangChain and Gemini because they allow me to build incredible applications very quickly.\"\n",
    "\n",
    "response = sequential_chain.invoke({\"text\": input_text})\n",
    "\n",
    "print(f\"Original Text: {input_text}\")\n",
    "\n",
    "print(f\"Final Summarized Response (in Tamil): {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e05cc32-b572-4bb2-9a94-56410fb4ed8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Topic: eco-friendly water bottles\n",
      "\n",
      "--- Sequential Chain Result ---\n",
      "Final Slogan: Here are a few slogan options for the \"Seedling Sip,\" playing with different angles:\n",
      "\n",
      "**Short & Catchy:**\n",
      "\n",
      "*   **Seedling Sip: Drink, Plant, Grow.** (Simple, action-oriented)\n",
      "*   **Sip Sustainably. Grow Beautifully.** (Highlights both aspects)\n",
      "*   **The Bottle That Blooms.** (Memorable and poetic)\n",
      "*   **Seedling Sip: Water Today, Flowers Tomorrow.** (Focuses on the future)\n",
      "*   **Plant the Change. One Sip at a Time.** (Emphasizes impact)\n",
      "\n",
      "**Inspiring & Benefit-Oriented:**\n",
      "\n",
      "*   **Seedling Sip: Quench Your Thirst, Grow the Future.** (Connects personal action to global impact)\n",
      "*   **Give Back to the Earth, One Sip at a Time.** (Focuses on environmental responsibility)\n",
      "*   **Seedling Sip: Where Hydration Meets Regeneration.** (Highlights the unique combination)\n",
      "*   **Drink Well. Plant Hope. Seedling Sip.** (Evokes positive emotions)\n",
      "*   **The Cycle of Life, in Every Sip.** (More philosophical and thought-provoking)\n",
      "\n",
      "**Unique & Intriguing:**\n",
      "\n",
      "*   **Seedling Sip: The Water Bottle That Lives On.** (Emphasizes the extended lifespan)\n",
      "*   **From Hydration to Habitat: The Seedling Sip Story.** (Suggests a narrative and journey)\n",
      "*   **Unbottle Your Potential. Plant a Seedling Sip.** (Plays on words and personal growth)\n",
      "*   **Seedling Sip: The End of Your Drink is Just the Beginning.** (Highlights the transformation)\n",
      "*   **Drink, Plant, Repeat. A Sustainable Cycle with Seedling Sip.** (Emphasizes the circularity)\n",
      "\n",
      "**My personal favorites from this list are:**\n",
      "\n",
      "*   **Seedling Sip: Drink, Plant, Grow.** (Simple and effective)\n",
      "*   **The Bottle That Blooms.** (Memorable and poetic)\n",
      "*   **Plant the Change. One Sip at a Time.** (Emphasizes impact)\n",
      "\n",
      "Ultimately, the best slogan will depend on your specific marketing strategy and target audience. Good luck!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# --- 1. Setup ---\n",
    "# Ensure your GOOGLE_API_KEY is set in your environment\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "parser = StrOutputParser()\n",
    "\n",
    "# --- 2. Define the First Chain: Idea Generator ---\n",
    "# This chain takes 'topic' as input and outputs a product 'idea'.\n",
    "idea_prompt = PromptTemplate.from_template(\n",
    "    \"Generate a unique and creative product idea based on the following topic: {topic}\"\n",
    ")\n",
    "idea_chain = idea_prompt | llm | parser\n",
    "\n",
    "# --- 3. Define the Second Chain: Slogan Creator ---\n",
    "# This chain takes the 'idea' (the output of the first chain) as input and outputs a 'slogan'.\n",
    "slogan_prompt = PromptTemplate.from_template(\n",
    "    \"Write a short, catchy, and inspiring slogan for this product: {idea}\"\n",
    ")\n",
    "slogan_chain = slogan_prompt | llm | parser\n",
    "\n",
    "# --- 4. Combine Chains Sequentially using LCEL ---\n",
    "# The output of 'idea_chain' is automatically passed as the input to 'slogan_chain'.\n",
    "# The initial input key is 'topic'.\n",
    "# The final output is the slogan from the last step.\n",
    "sequential_chain = idea_chain | slogan_chain\n",
    "\n",
    "# --- 5. Invoke the Chain ---\n",
    "input_topic = \"eco-friendly water bottles\"\n",
    "print(f\"Input Topic: {input_topic}\\n\")\n",
    "\n",
    "# Run the entire pipeline with a single invocation\n",
    "response = sequential_chain.invoke({\"topic\": input_topic})\n",
    "\n",
    "print(\"--- Sequential Chain Result ---\")\n",
    "print(f\"Final Slogan: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b23ee2b-02d7-4c8b-aaa4-9639d95f96ee",
   "metadata": {},
   "source": [
    "### Map-Reduce Summarization Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d8924256-2152-4543-b33b-5452a460a237",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Splitter <langchain_text_splitters.character.RecursiveCharacterTextSplitter object at 0x000001BDE0165480>\n",
      "\n",
      "Split documet [Document(metadata={}, page_content='LangChain is a powerful open-source framework that helps developers \\nbuild applications using large language models. It supports memory,'), Document(metadata={}, page_content='agents, retrieval-augmented generation, document processing, and more. \\nThe framework is widely used for chatbots, automation, RAG systems,'), Document(metadata={}, page_content='workflow orchestration, and AI-driven enterprise applications.')]\n",
      " \n",
      "FINAL SUMMARY:\n",
      "\n",
      "LangChain is an open-source framework that helps developers build LLM-powered applications like chatbots using tools for memory, automation, document processing, and RAG. It also supports AI-driven enterprise applications and workflow orchestration.\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# 2. Long text\n",
    "text = \"\"\"\n",
    "LangChain is a powerful open-source framework that helps developers \n",
    "build applications using large language models. It supports memory, \n",
    "agents, retrieval-augmented generation, document processing, and more. \n",
    "The framework is widely used for chatbots, automation, RAG systems, \n",
    "workflow orchestration, and AI-driven enterprise applications.\n",
    "\"\"\"\n",
    "\n",
    "# 3. Convert into Documents\n",
    "docs = [Document(page_content=text)]\n",
    "\n",
    "# 4. Split into chunks (Map phase)\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=150, chunk_overlap=20)\n",
    "split_docs = splitter.split_documents(docs)\n",
    "\n",
    "print(\"\\nSplitter\",splitter)\n",
    "print(\"\\nSplit documet\",split_docs)\n",
    "\n",
    "# 5. Map-Reduce chain\n",
    "chain = load_summarize_chain(\n",
    "    llm,\n",
    "    chain_type=\"map_reduce\" # chain_type=\"stuff\"\n",
    ")\n",
    "\n",
    "# 6. Run summarizer\n",
    "summary = chain.invoke({\"input_documents\": split_docs})\n",
    "\n",
    "print(\" \\nFINAL SUMMARY:\\n\")\n",
    "print(summary[\"output_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2306ae2-6579-405d-9187-f5456dd4b6be",
   "metadata": {},
   "source": [
    "### STUFF SUMMARIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "defdb68b-091b-41e0-a77b-7dbb274b9ae6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain streamlines the process of building applications powered by large language models (LLMs) by providing tools and abstractions that make it easier to manage and connect different components like data sources, models, and prompts.\n"
     ]
    }
   ],
   "source": [
    "# LCEL style\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.schema.runnable import RunnableMap, RunnableLambda\n",
    "from langchain.schema import Document\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\")\n",
    "\n",
    "# Combine text\n",
    "def combine_docs(docs):\n",
    "    return \"\\n\\n\".join([d.page_content for d in docs])\n",
    "\n",
    "# LCEL pipeline\n",
    "stuff_chain = (\n",
    "    RunnableMap({\"text\": combine_docs})| (lambda x: f\"Summarize this:\\n\\n{x['text']}\")| llm\n",
    ")\n",
    "\n",
    "docs = [Document(page_content=\"LangChain simplifies LLM workflow engineering...\")]\n",
    "\n",
    "summary = stuff_chain.invoke(docs)\n",
    "\n",
    "print(summary.content)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7cced91-8a37-43b3-8ebe-bcf7202955be",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
