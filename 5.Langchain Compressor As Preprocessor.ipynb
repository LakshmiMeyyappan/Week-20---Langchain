{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0914c7e2-3451-4c1e-a82d-0890aad98abf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compressed Summary:\n",
      "- LangChain helps build LLM-powered apps with memory and agents.\n",
      "- Agents in LangChain use tools to answer questions.\n",
      "\n",
      " ANSWER WITH OUT COMPRESSION\n",
      "==============================\n",
      "The main idea of the document is that LangChain is a framework for building applications powered by Large Language Models (LLMs), and it provides features like memory and agents that can use tools to answer questions.\n",
      "Length before compression 217\n",
      "Token count without compression 41\n",
      "\n",
      " ANSWER WITH COMPRESSION\n",
      "============================\n",
      "The main idea of the document is that LangChain is a framework for building applications powered by Large Language Models (LLMs), and it achieves this by providing tools like memory and agents, where agents use tools to answer questions.\n",
      "Length after compression 237\n",
      "Token count after compression 45\n"
     ]
    }
   ],
   "source": [
    "# 1. Imports\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain.schema.document import Document\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
    "\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "\n",
    "# 1. Prepare documents\n",
    "documents = [Document(page_content=\"LangChain helps build LLM-powered apps with memory and agents.\", metadata={\"id\": \"1\"}),\n",
    "        Document(page_content=\"Agents in LangChain use tools to answer questions.\", metadata={\"id\": \"2\"})]\n",
    "\n",
    "# 2. Setup child splitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "\n",
    "# 5. Compress the documents using LLMChainExtractor\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compressed_docs = compressor.compress_documents(documents, query=\"Summarize the key points\")\n",
    "\n",
    "print(\"Compressed Summary:\")\n",
    "for doc in compressed_docs:\n",
    "    print(\"-\", doc.page_content)\n",
    "\n",
    "# 6. Now ask a question about the compressed content using a custom LLMChain\n",
    "qa_prompt = PromptTemplate.from_template(\n",
    "    \"Given the context below, answer the question:\\n\\nContext:\\n{context}\\n\\nQuestion: {question}\"\n",
    ")\n",
    "qa_chain = LLMChain(llm=llm, prompt=qa_prompt)\n",
    "\n",
    "query = \"What is the main idea of the document?\"\n",
    "\n",
    "# Response without Compression\n",
    "response_without = qa_chain.invoke({\n",
    "    \"context\": \"\\n\".join([doc.page_content for doc in documents]),\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "#Response with compression\n",
    "response_with = qa_chain.invoke({\n",
    "    \"context\": \"\\n\".join([doc.page_content for doc in compressed_docs]),\n",
    "    \"question\": query\n",
    "})\n",
    "\n",
    "\n",
    "print(\"\\n ANSWER WITH OUT COMPRESSION\")\n",
    "print(\"==============================\")\n",
    "print(response_without[\"text\"])\n",
    "print(\"Length before compression\",len(response_without[\"text\"]))\n",
    "print(\"Token count without compression\",llm.get_num_tokens(response_without[\"text\"]))\n",
    "\n",
    "\n",
    "print(\"\\n ANSWER WITH COMPRESSION\")\n",
    "print(\"============================\")\n",
    "print(response_with[\"text\"])\n",
    "print(\"Length after compression\",len(response_with[\"text\"]))\n",
    "print(\"Token count after compression\",llm.get_num_tokens(response_with[\"text\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50e7ddb9-976f-4bd0-8106-114c3d50bfa4",
   "metadata": {},
   "source": [
    "Why Length and Tokens Changed Even After Compression?\n",
    "*****************************************************\n",
    "\n",
    "Important:\n",
    "\n",
    "LLMChainExtractor compresses documents, NOT the final answer.\n",
    "\n",
    "The answer may become longer because the LLM explains more clearly even if the input summary is shorter.\n",
    "\n",
    "So the compression reduces input text, not necessarily output answer length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f43766b-a979-4557-a877-b394d3124c1c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
