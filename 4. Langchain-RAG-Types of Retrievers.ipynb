{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce7bae91-dc16-4480-9598-c131e6058f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc912b42-6fbf-4e86-88ef-12b8b835e3b3",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "def41213-7462-4afa-a8a2-8e1224f26da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'GoogleGenerativeAIEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000001E7FFDA4050> search_kwargs={}\n",
      "Answer:\n",
      " LangChain is an open-source framework created by Harrison Chase that simplifies the development of applications powered by large language models (LLMs). It provides tools and standard interfaces for integrating with various LLMs and external resources, allowing developers to build complex and modular AI-driven solutions like chatbots, document Q&A systems, and AI workflows. Its core functionalities include Chains, Agents, Prompt Management, Memory, and Integrations.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "texts = splitter.split_text(text_data)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts,embeddings)\n",
    "\n",
    "#vectorstore = Chroma.from_texts(texts, embedding=embeddings, persist_directory=\"./chroma_db\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(retriever)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"     # simplest style: stuff retrieved chunks directly into prompt\n",
    ")\n",
    "\n",
    "query = \"Summarize what LangChain is and its core purpose.\"\n",
    "answer = qa_chain.invoke({\"query\": query})\n",
    "print(\"Answer:\\n\", answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78f32dda-d912-4f64-a1b3-d2be22b41ce1",
   "metadata": {},
   "source": [
    "## TYPES OF RETRIEVERS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a9d7eb-ff42-4ae9-ae3e-1ef969c19239",
   "metadata": {},
   "source": [
    "### ContextualCompressionRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "549124a9-e8dc-41f5-87f8-bae59f8f6271",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_31712\\3954767240.py:11: LangChainDeprecationWarning: The method `BaseRetriever.get_relevant_documents` was deprecated in langchain-core 0.1.46 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = compression_retriever.get_relevant_documents(\"Explain LangChain in short\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides a standard interface for integrating with various LLMs and external tools, enabling the creation of complex, modular, and advanced AI-driven solutions.\n",
      "- LangChain is a framework for building applications with LLMs.LangChain was created by Harrison Chase.LangChain supports RAG, agents, memory, tools, and more.Itâ€™s commonly used in chatbots, document Q&A, and AI workflow\n",
      "- LangChain (Core Framework): \n",
      "This is the foundation, providing the core abstractions and interfaces for building LLM applications. It includes functionalities for:\n",
      "Chains: Combining LLMs with other components (like data sources, tools) into sequences of operations.\n",
      "Agents: Allowing LLMs to interact with their environment, use tools, and perform multi-step reasoning.\n",
      "Prompt Management: Tools for creating, managing, and optimizing prompts for LLMs.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# Inspect retrieved compressed docs\n",
    "result = compression_retriever.get_relevant_documents(\"Explain LangChain in short\")\n",
    "\n",
    "for doc in result:\n",
    "    print(\"-\",doc.page_content)\n",
    "\n",
    "# Or use in a QA chain\n",
    "#from langchain.chains import RetrievalQA\n",
    "#qa = RetrievalQA.from_chain_type(llm=llm, retriever=compression_retriever)\n",
    "#qa.invoke({\"query\": \"Explain LangChain in short\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "768a8772-7e81-40a3-a637-612edf8ff2fe",
   "metadata": {},
   "source": [
    "### Integration with ConversationalRetrievalChain- Without agent mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2646914-cad7-49d6-b730-0aad72269718",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ConversationalRetrievalChain:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_31712\\975304748.py:16: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides the core abstractions and interfaces for building LLM applications, such as chains, agents, and prompt management tools. It supports RAG, agents, memory, tools, and more, and it's commonly used in chatbots, document Q&A, and AI workflows. LangChain was created by Harrison Chase.\n",
      "LangChain was created by Harrison Chase.\n"
     ]
    }
   ],
   "source": [
    "#Integration with ConversationalRetrievalChain- Without agent mode\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "# âœ… ContextualCompressionRetriever\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "# âœ… Memory\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "# âœ… Chain\n",
    "rag_chain = ConversationalRetrievalChain.from_llm(\n",
    "    llm=llm,\n",
    "    retriever=compression_retriever,\n",
    "    memory=memory\n",
    ")\n",
    "\n",
    "# ðŸ§ª Ask a few questions\n",
    "print(\"\\n ConversationalRetrievalChain:\")\n",
    "print(rag_chain.invoke({\"question\": \"What is LangChain?\"})[\"answer\"])\n",
    "print(rag_chain.invoke({\"question\": \"Who created it?\"})[\"answer\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc35f1b-fbbe-4245-aa84-93aa2fd3413c",
   "metadata": {},
   "source": [
    "### Integration with Agent (using Tool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d02667d4-f81f-452e-baeb-e18e13e8a8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Agent Conversation:\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides the core abstractions and interfaces for building LLM applications, such as chains, agents, and prompt management tools. It supports RAG, agents, memory, tools, and more, and it's commonly used in chatbots, document Q&A, and AI workflows.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs). It provides the core abstractions and interfaces for building LLM applications, such as chains, agents, and prompt management tools. It supports RAG, agents, memory, tools, and more, and it's commonly used in chatbots, document Q&A, and AI workflows.\n",
      "```\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? No\n",
      "AI: LangChain was created by Harrison Chase.\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "LangChain was created by Harrison Chase.\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "#Integration into Agent (with Tool)\n",
    "\n",
    "from langchain.agents import initialize_agent, AgentType\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "# 5. Wrap RAG as a StructuredTool\n",
    "def rag_tool_fn(question: str) -> str:\n",
    "    return rag_chain.invoke({\n",
    "        \"question\": question,\n",
    "        \"chat_history\": [memory.chat_memory.messages ]\n",
    "    })[\"answer\"]\n",
    "\n",
    "# Structured Tool\n",
    "rag_tool = StructuredTool.from_function(\n",
    "    name=\"RAG_Tool\",\n",
    "    description=\"Answer LangChain-related questions with context.\",\n",
    "    func=rag_tool_fn\n",
    ")\n",
    "\n",
    "# Agent with memory\n",
    "agent = initialize_agent(\n",
    "    tools=[rag_tool],\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Ask via agent\n",
    "print(\"\\n Agent Conversation:\")\n",
    "print(agent.run(\"What is LangChain?\"))\n",
    "print(agent.run(\"Who created it?\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1abfbc-f10a-4403-876d-23e79a9baa6e",
   "metadata": {},
   "source": [
    "### MultiQueryRetriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d83ad2e-46eb-44ae-b6af-c18da8e140fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG pipeline (multiqueryretriever)\n",
      "Answer: LangChain was created by Harrison Chase. It is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).\n",
      "\n",
      "Key features of LangChain include:\n",
      "\n",
      "*   **Chains:** Combining LLMs with other components (like data sources, tools) into sequences of operations.\n",
      "*   **Agents:** Allowing LLMs to interact with their environment, use tools, and perform multi-step reasoning.\n",
      "*   **Prompt Management:** Tools for creating, managing, and optimizing prompts for LLMs.\n",
      "*   **Memory:** Storing and retrieving conversational history for stateful interactions.\n",
      "*   **Integrations:** Connecting with various LLMs, vector stores, document loaders, and other tools.\n",
      "\n",
      "It supports RAG (Retrieval-Augmented Generation). It is commonly used in chatbots, document Q&A, and AI workflow.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "rag_multi = RetrievalQA.from_chain_type(\n",
    "    llm=llm, \n",
    "    retriever=multi_query_retriever,\n",
    "    return_source_documents = True)\n",
    "\n",
    "print(\"RAG pipeline (multiqueryretriever)\")\n",
    "res = rag_multi.invoke({\"query\":\"tell me about langchain creator and its features\"})\n",
    "print(\"Answer:\",res[\"result\"])\n",
    "\n",
    "#print(\"\\n sources\")\n",
    "#for doc in res[\"source_documents\"]:\n",
    "#    print(\"-\",doc.page_content[:200])\n",
    "\n",
    "#docs = multi_query_retriever.get_relevant_documents(\"Who developed LangChain?\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee7a923-78d5-4631-b08d-d599e9483fcd",
   "metadata": {},
   "source": [
    "### PARENT DOCUMENT RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2dee9edf-3609-4e14-8d4a-bdd9bc226f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Retrieved Doc: Agents in LangChain use tools to answer questions.\n",
      " Retrieved Doc: LangChain helps build LLM-powered apps with memory and agents.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# 1. Prepare documents\n",
    "docs = [Document(page_content=\"LangChain helps build LLM-powered apps with memory and agents.\", metadata={\"id\": \"1\"}),\n",
    "        Document(page_content=\"Agents in LangChain use tools to answer questions.\", metadata={\"id\": \"2\"})]\n",
    "\n",
    "# 2. Setup child splitter\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=100, chunk_overlap=20)\n",
    "\n",
    "# 3. Setup vectorstore for children\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts,embeddings)\n",
    "\n",
    "# 4. Parent retriever\n",
    "retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=InMemoryStore(),  # Stores parent docs\n",
    "    child_splitter=child_splitter\n",
    ")\n",
    "\n",
    "# 5. Add documents\n",
    "retriever.add_documents(docs)\n",
    "\n",
    "# 6. Retrieve\n",
    "results = retriever.get_relevant_documents(\"What are agents?\")\n",
    "for doc in results:\n",
    "    print(\" Retrieved Doc:\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d0b3a4-c9bb-4f63-9f4d-6e0ea47f0b29",
   "metadata": {},
   "source": [
    "### BM25 RETRIEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "927618d2-3d4a-442e-bfd4-b462da8f81ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install rank_bm25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c891fb28-5c86-4491-858f-2028b8e34d48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESULT: [Document(metadata={}, page_content='BM25 is a classical retrieval method.'), Document(metadata={}, page_content='Vector search is powerful.'), Document(metadata={}, page_content='LangChain enables LLM applications.')]\n",
      " BM25 Result: BM25 is a classical retrieval method.\n",
      " BM25 Result: Vector search is powerful.\n",
      " BM25 Result: LangChain enables LLM applications.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import BM25Retriever\n",
    "from langchain.schema.document import Document\n",
    "\n",
    "# Create simple text docs\n",
    "docs = [\n",
    "    Document(page_content=\"LangChain enables LLM applications.\"),\n",
    "    Document(page_content=\"Vector search is powerful.\"),\n",
    "    Document(page_content=\"BM25 is a classical retrieval method.\")\n",
    "]\n",
    "\n",
    "# Create BM25 retriever\n",
    "bm25_retriever = BM25Retriever.from_documents(docs)\n",
    "\n",
    "# Retrieve\n",
    "results = bm25_retriever.get_relevant_documents(\"embedding\")\n",
    "print(\"RESULT:\",results)\n",
    "for doc in results:\n",
    "    print(\" BM25 Result:\", doc.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d03eb76-770e-4161-a52c-44ac67a67f3f",
   "metadata": {},
   "source": [
    "### Ensemble Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dc8255b8-237b-44a6-ae40-dc0e555feff4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'What is LangChain used for?',\n",
       " 'result': 'LangChain is used for building applications powered by large language models (LLMs). It provides tools to create chatbots, document Q&A systems, and AI workflows. It supports RAG (Retrieval-Augmented Generation), agents, memory, and tools. It helps combine LLMs with other components like data sources into sequences of operations.'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.retrievers.multi_query import MultiQueryRetriever\n",
    "\n",
    "#retriever1 = vectorstore.as_retriever(search_kwargs={\"k\": 3})\n",
    "#retriever2 = multi_query_retriever\n",
    "\n",
    "multi_query_retriever = MultiQueryRetriever.from_llm(\n",
    "    retriever=vectorstore.as_retriever(),\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=vectorstore.as_retriever()\n",
    ")\n",
    "\n",
    "ensemble = EnsembleRetriever(\n",
    "    retrievers=[multi_query_retriever, compression_retriever],\n",
    "    weights=[0.5, 0.5]\n",
    ")\n",
    "qa = RetrievalQA.from_chain_type(llm=llm, retriever=ensemble)\n",
    "qa.invoke({\"query\": \"What is LangChain used for?\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "37bf65bf-8b26-4bd9-9461-1b8159a54f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ensemble Result: Vector search is powerful.\n",
      "Ensemble Result: LangChain enables LLM applications.\n",
      "Ensemble Result: BM25 is a classical retrieval method.\n"
     ]
    }
   ],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embedding_fn = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "vectorstore = FAISS.from_documents(docs, embedding_fn)\n",
    "vector_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "ensemble = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, vector_retriever],\n",
    "    weights=[0.3, 0.7]\n",
    ")\n",
    "\n",
    "results = ensemble.get_relevant_documents(\"vector\")\n",
    "for doc in results:\n",
    "    print(\"Ensemble Result:\", doc.page_content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b68d0a8-d429-4852-a787-92a12229b00b",
   "metadata": {},
   "source": [
    "### TimeWeightVectorStore Retriever"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2679c431-765e-4de8-9ab3-c9faa412138f",
   "metadata": {},
   "source": [
    "#### There is no built-in TimeWeightedVectorStoreRetriever in LangChain 0.3.25. - Use custom time decay / use Enseble Retriever / Switch tolanggraph memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8e097fa8-441d-42b8-9d2b-63d70d98d890",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Doc: Vector search improves relevance.\n",
      "Timestamp: 2025-11-14 13:08:40.332532\n",
      "Doc: LangChain helps build LLM apps.\n",
      "Timestamp: 2025-11-14 12:43:40.332478\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain.schema import Document\n",
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "# 1. Embeddings\n",
    "embedding = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "# 2. Store docs with timestamps\n",
    "docs = [\n",
    "    Document(\n",
    "        page_content=\"LangChain helps build LLM apps.\",\n",
    "        metadata={\"time\": datetime.now() - timedelta(minutes=30)}  # 30 mins old\n",
    "    ),\n",
    "    Document(\n",
    "        page_content=\"Vector search improves relevance.\",\n",
    "        metadata={\"time\": datetime.now() - timedelta(minutes=5)}  # 5 mins old = fresher\n",
    "    )\n",
    "]\n",
    "\n",
    "# 3. Build vectorstore\n",
    "vectorstore = FAISS.from_documents(docs, embedding)\n",
    "\n",
    "# 4. Normal retriever\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 2})\n",
    "\n",
    "# 5. Apply TIME WEIGHT after vector search\n",
    "def time_weighted_results(query):\n",
    "    results = retriever.get_relevant_documents(query)\n",
    "    now = datetime.now()\n",
    "\n",
    "    scored = []\n",
    "    for d in results:\n",
    "        age_minutes = (now - d.metadata[\"time\"]).total_seconds() / 60\n",
    "        time_score = 1 / (1 + age_minutes)   # Smaller age â†’ bigger score\n",
    "        scored.append((time_score, d))\n",
    "\n",
    "    scored.sort(reverse=True, key=lambda x: x[0])\n",
    "    return [d for (_, d) in scored]\n",
    "\n",
    "\n",
    "# TEST\n",
    "results = time_weighted_results(\"What is LangChain?\")\n",
    "for r in results:\n",
    "    print(\"Doc:\", r.page_content)\n",
    "    print(\"Timestamp:\", r.metadata[\"time\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c2b50e5-89d7-48ff-9567-dd92bc81c9eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
