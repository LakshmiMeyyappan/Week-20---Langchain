{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c5ad6b2d-d414-4fbb-a937-ff65cff8c50c",
   "metadata": {},
   "source": [
    "### Chat Implementation with File Chat Message history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1613f76-e7db-42a8-a031-ffa7f0f1028c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat started (type 'quit' or 'exit' to end the session).\n",
      "------------------------------\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is langchain\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo: Langchain is a framework designed to simplify the creation of applications using large language models (LLMs). It provides tools and abstractions that help you connect LLMs to other data sources and build more complex and useful AI applications.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  explain its ecosystem\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo: Okay, so the Langchain ecosystem is made up of several key parts that all work together:\n",
      "\n",
      "*   **LangChain Libraries:** These are the core Python and JavaScript libraries with modules for models, prompts, chains, data connection, agents, and memory.\n",
      "*   **LangChain Templates:** A collection of easily deployable reference architectures for a variety of tasks.\n",
      "*   **LangChain Hub:** A central place to discover and share prompts, chains, and agents.\n",
      "*   **Integrations:** LangChain supports integrations with numerous third-party services like vector databases, APIs, and other tools.\n",
      "*   **Community:** A large and active community contributing to the project through code, documentation, and support.\n",
      "\n",
      "In short, it's a comprehensive set of tools, integrations, and resources designed to make building LLM-powered applications easier and more efficient.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  explain its agent types\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo: Alright, let's break down Langchain agent types! Agents use LLMs to decide what actions to take. Here are a few common types:\n",
      "\n",
      "*   **Zero-shot ReAct:** The agent takes action based on a single input.\n",
      "*   **ReAct:** This is similar to zero-shot, but the agent can \"think\" (reason) about what to do. It's more powerful for complex tasks.\n",
      "*   **Self-Ask with Search:** This agent can specifically ask search queries to find information.\n",
      "*   **Conversational:** Designed for conversational scenarios, remembering past interactions.\n",
      "*   **Plan-and-execute agents:** These agents plan out the entire sequence of actions to take, and then execute the plan.\n",
      "\n",
      "Each agent type is suited for different tasks, depending on the complexity and need for reasoning or memory.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  explain its types of memory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo: Okay, let's talk Langchain memory! Memory helps agents \"remember\" previous interactions. Here are some common types:\n",
      "\n",
      "*   **ConversationBufferMemory:** Simplest form, it stores the entire conversation history.\n",
      "*   **ConversationBufferWindowMemory:** Stores a rolling window of the most recent interactions.\n",
      "*   **ConversationSummaryMemory:** Summarizes the conversation over time to keep the memory concise.\n",
      "*   **ConversationSummaryBufferMemory:** Combines the buffer and summary approaches. It uses a buffer initially, then summarizes when the buffer gets too large.\n",
      "*   **ConversationKnowledgeGraphMemory:** Uses a knowledge graph to store and retrieve information from the conversation.\n",
      "\n",
      "The best type of memory depends on the specific application and how much context the agent needs to retain!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  what is map reduce and runnable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Geo: Okay, let's clarify Map Reduce and Runnable in the context of Langchain:\n",
      "\n",
      "*   **Map Reduce:** In Langchain, Map Reduce is a type of chain often used for processing large documents. It works by:\n",
      "    *   **Mapping:** Splitting the document into chunks and processing each chunk independently with an LLM.\n",
      "    *   **Reducing:** Combining the results from each chunk into a single, final output using another LLM call. This is great for summarizing long texts or answering questions based on a large corpus of data.\n",
      "\n",
      "*   **Runnable:** In Langchain, a \"Runnable\" is a standard interface that represents a callable unit. It's a core concept that allows you to easily chain together different components like models, prompts, and other runnables. Anything that implements the Runnable interface can be invoked, composed, and streamed, making it super flexible for building complex workflows.\n",
      "\n",
      "In short, Map Reduce is a specific type of chain for processing large amounts of data, while Runnable is a more general interface for creating composable and executable units within Langchain. They can be used together to build powerful and modular LLM applications!\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You:  exit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Session ended. Your history is saved to: persistent_chat_log.json\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain_community.chat_message_histories import FileChatMessageHistory\n",
    "from langchain_core.runnables import ConfigurableFieldSpec\n",
    "\n",
    "# --- 1. Setup ---\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "\n",
    "FILE_PATH = \"persistent_chat_log.json\"\n",
    "SESSION_ID = \"main-user-session\"\n",
    "\n",
    "# --- 2. Define the LLM and Chain (The Logic) ---\n",
    "\n",
    "# Define the Prompt with the critical MessagesPlaceholder\n",
    "prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"You are a helpful and friendly assistant named Geo. Keep your answers concise and conversational.\"),\n",
    "        # This is where the chat history from the FileChatMessageHistory will be injected\n",
    "        MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "        # This is where the current user input will be injected\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Define the LLM and the Core Chain (Prompt | LLM | Parser)\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.3)\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# --- 3. Define the History Manager Function ---\n",
    "\n",
    "# This function tells RunnableWithMessageHistory how to get the correct history object\n",
    "def get_session_history(session_id: str) -> FileChatMessageHistory:\n",
    "    \"\"\"Uses the session ID to create a unique FileChatMessageHistory object.\"\"\"\n",
    "    # We use a static file name here, but in a real app, you might use f\"{session_id}.json\"\n",
    "    return FileChatMessageHistory(file_path=FILE_PATH)\n",
    "\n",
    "# --- 4. Wrap the Chain with History ---\n",
    "\n",
    "# RunnableWithMessageHistory wraps your chain and manages the chat_history variable\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    runnable=chain,\n",
    "    get_session_history=get_session_history,\n",
    "    # Maps the key \"input\" from the user to the prompt variable {input}\n",
    "    input_messages_key=\"input\", \n",
    "    # Maps the key \"chat_history\" from the history function to the MessagesPlaceholder\n",
    "    history_messages_key=\"chat_history\",\n",
    "    # Define a configurable field spec to ensure the session_id is always present\n",
    "    # We use a fixed ID here for simplicity\n",
    "    configurable={\n",
    "        \"session_id\": ConfigurableFieldSpec(\n",
    "            id=\"session_id\",\n",
    "            annotation=str,\n",
    "            default=SESSION_ID,\n",
    "            is_shared=True,\n",
    "        )\n",
    "    }\n",
    ")\n",
    "\n",
    "# --- 5. The Chat Loop (Getting User Input) ---\n",
    "\n",
    "print(\"Chat started (type 'quit' or 'exit' to end the session).\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"You: \")\n",
    "        \n",
    "        # Check for exit command\n",
    "        if user_input.lower() in [\"quit\", \"exit\"]:\n",
    "            print(\"\\n Session ended. Your history is saved to:\", FILE_PATH)\n",
    "            break\n",
    "            \n",
    "        # Invoke the chain with the user input and the session config\n",
    "        response = chain_with_history.invoke(\n",
    "            {\"input\": user_input},\n",
    "            # Pass the session_id so the history manager knows which file to use\n",
    "            config={\"configurable\": {\"session_id\": SESSION_ID}}\n",
    "        )\n",
    "        \n",
    "        print(f\"Geo: {response}\")\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"\\n Session ended by user (Ctrl+C). Your history is saved to:\", FILE_PATH)\n",
    "        break\n",
    "    except Exception as e:\n",
    "        print(f\"\\nAn error occurred: {e}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefdf833-840f-4f06-9556-e23ce1d7c8b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
