{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ff13d0-5469-4c59-8662-d7029c2ade02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LangChain is a framework for developing applications powered by language models.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Load API key\n",
    "load_dotenv()\n",
    "API_KEY = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You answer in one short sentence\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "print(chain.invoke({\"question\": \"What is LangChain?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae682f5-dfc2-405a-a9ac-14deeff0196d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install tavily-python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b5146f-9393-42a6-ac4b-0c943ec181ad",
   "metadata": {},
   "source": [
    "### Creating Multiple Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "04d0b53d-5bb7-4317-86fe-e4df77244de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Simple QA tool: LangChain is a framework for developing applications powered by language models.\n",
      "\n",
      " Summarizer Tool: LangChain helps build applications using large language models by connecting them to external data and tools.\n",
      "\n",
      " Web search Tool: In this video, I cover LangChain's recent announcements about raising money at over a $1.25 valuation and the launch of LangChain and\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "from tavily import TavilyClient\n",
    "\n",
    "#TAVILY_API_KEY = os.getenv(\"TAVILY_API_KEY\")\n",
    "tavily = TavilyClient(api_key=os.getenv(\"TAVILY_API_KEY\"))\n",
    "\n",
    "@tool\n",
    "def qa_tool(question: str) -> str:\n",
    "    \"\"\"Answer in one short sentence.\"\"\"\n",
    "    return chain.invoke({\"question\": question})\n",
    "\n",
    "@tool\n",
    "def summarize_tool(question: str) -> str:\n",
    "    \"\"\"Summarize input text.\"\"\"\n",
    "    return chain.invoke({\"question\": f\"Summarize this:\\n\\n{text}\"})\n",
    "\n",
    "@tool\n",
    "def search_tool(query: str) -> str:\n",
    "    \"\"\"Quick internet search (best single answer).\"\"\"\n",
    "    results = tavily.search(query=query, max_results=1)\n",
    "    return results[\"results\"][0][\"content\"]\n",
    "\n",
    "\n",
    "print(\"\\n Simple QA tool:\",qa_tool.invoke(\"What is LangChain?\"))\n",
    "\n",
    "text=\"LangChain is a framework for developing applications powered by large language models, often connecting them with external data and tools.\"\n",
    "\n",
    "print(\"\\n Summarizer Tool:\",summarize_tool.invoke(text))\n",
    "\n",
    "#search_text=\"Give the latest updtes on langchain?\"\n",
    "\n",
    "#print(\"\\n Web search Tool:\",search_tool.invoke(search_text))\n",
    "\n",
    "search_text = \"Give the latest updates on LangChain?\"\n",
    "\n",
    "print(\"\\n Web search Tool:\", search_tool.invoke({\"query\": search_text}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44fa1389-8a85-4692-a78a-35271abb084f",
   "metadata": {},
   "source": [
    "### AGENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5894624f-5e2d-4674-a824-225a8b7a5bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4384\\320658337.py:4: LangChainDeprecationWarning: LangChain agents will continue to be supported, but it is recommended for new use cases to be built with LangGraph. LangGraph offers a more flexible and full-featured framework for building agents, including support for tool-calling, persistence of state, and human-in-the-loop workflows. For details, refer to the `LangGraph documentation <https://langchain-ai.github.io/langgraph/>`_ as well as guides for `Migrating from AgentExecutor <https://python.langchain.com/docs/how_to/migrate_agent/>`_ and LangGraph's `Pre-built ReAct agent <https://langchain-ai.github.io/langgraph/how-tos/create-react-agent/>`_.\n",
      "  react_agent = initialize_agent(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI need to summarize what Langchain is. I can use the summarize tool for this.\n",
      "Action: summarize_tool\n",
      "Action Input: Langchain\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for building LLM-powered applications with external data and tools.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI now know the final answer\n",
      "Final Answer: LangChain is a framework for building LLM-powered applications with external data and tools.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Summarise about langchain', 'output': 'LangChain is a framework for building LLM-powered applications with external data and tools.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.agents import initialize_agent, AgentType\n",
    "\n",
    "tools = [qa_tool, summarize_tool,search_tool]\n",
    "react_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION,  # MAIN REACT TYPE\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(react_agent.invoke({\"input\": \"Summarise about langchain\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83f3da-365c-43e9-9a91-b085ff255858",
   "metadata": {},
   "source": [
    "### CONVERSATIONAL REACT DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c726493c-77dc-4b65-ac79-d476a7d05f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4384\\712296217.py:4: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: search_tool\n",
      "Action Input: langgraph definition\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'what is langgraph?', 'chat_history': [HumanMessage(content='what is langgraph?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.', additional_kwargs={}, response_metadata={})], 'output': 'LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```\n",
      "Thought: Do I need to use a tool? Yes\n",
      "Action: search_tool\n",
      "Action Input: LangGraph ecosystem\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mLangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner. By managing the flow of data and the sequence of operations, LangGraph allows developers to focus on the high-level logic of their applications rather than the intricacies of agent coordination. Whether you need a chatbot that can handle various types of user requests or a multi-agent system that performs complex tasks, LangGraph provides the tools to build exactly what you need. LangGraph significantly simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mDo I need to use a tool? No\n",
      "AI: LangGraph exists within the LangChain ecosystem and simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions. It allows developers to define, coordinate, and execute multiple LLM agents or chains in a structured and efficient manner.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'Explain its ecosystem', 'chat_history': [HumanMessage(content='what is langgraph?', additional_kwargs={}, response_metadata={}), AIMessage(content='LangGraph is a library within the LangChain ecosystem that provides a framework for defining, coordinating, and executing multiple LLM agents (or chains) in a structured and efficient manner.', additional_kwargs={}, response_metadata={}), HumanMessage(content='Explain its ecosystem', additional_kwargs={}, response_metadata={}), AIMessage(content='LangGraph exists within the LangChain ecosystem and simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions. It allows developers to define, coordinate, and execute multiple LLM agents or chains in a structured and efficient manner.', additional_kwargs={}, response_metadata={})], 'output': 'LangGraph exists within the LangChain ecosystem and simplifies the development of complex LLM applications by providing a structured framework for managing state and coordinating agent interactions. It allows developers to define, coordinate, and execute multiple LLM agents or chains in a structured and efficient manner.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "#agent = create_tool_calling_agent(llm, tools)\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CONVERSATIONAL_REACT_DESCRIPTION,  # MAIN REACT TYPE\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "#agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)\n",
    "\n",
    "print(react_agent.invoke({\"input\": \"what is langgraph?\"}))\n",
    "print(react_agent.invoke({\"input\": \"Explain its ecosystem\"}))   # remembers context\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57abc8c6-d922-4166-93e2-8276aa6f9920",
   "metadata": {},
   "source": [
    "### CHAT CONVERSATIONAL REACT DESCRIPTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "344bae81-8f66-4c78-8e45-b53e8549f823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"summarize_tool\",\n",
      "    \"action_input\": \"Langchain is a framework for developing applications powered by language models. It enables the creation of applications that are data-aware and agentic.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mLangChain is a framework for building LLM-powered applications.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"search_tool\",\n",
      "    \"action_input\": \"who created langchain\"\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[38;5;200m\u001b[1;3mHarrison Chase, the visionary co-founder of LangChain, has redefined how developers harness Language Learning Models (LLMs) through a revolutionary open-source framework. His work at LangChain not only empowers developers to build context-aware applications but also sets new standards for collaboration in the AI ecosystem. The idea for LangChain took root in 2022 when Chase recognized a recurring challenge: developers were struggling to build reliable LLM applications due to fragmented tools and insufficient abstractions. Beyond LangChain, Chase remains passionate about democratizing AI, ensuring that innovative tools are accessible to developers of all skill levels. Through LangChain, he has not only simplified the mechanics of LLM development but also fostered a culture of collaboration that continues to propel the AI community forward.\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "    \"action\": \"Final Answer\",\n",
      "    \"action_input\": \"LangChain is a framework for building LLM-powered applications, and it was created by Harrison Chase.\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "{'input': 'summarize langchain in 2 lines then tell me who created it', 'chat_history': [HumanMessage(content='summarize langchain in 2 lines then tell me who created it', additional_kwargs={}, response_metadata={}), AIMessage(content='LangChain is a framework for building LLM-powered applications, and it was created by Harrison Chase.', additional_kwargs={}, response_metadata={})], 'output': 'LangChain is a framework for building LLM-powered applications, and it was created by Harrison Chase.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.agents import create_tool_calling_agent, AgentExecutor\n",
    "\n",
    "memory = ConversationBufferMemory(memory_key=\"chat_history\", return_messages=True)\n",
    "\n",
    "#agent = create_tool_calling_agent(llm, tools)\n",
    "\n",
    "react_agent = initialize_agent(\n",
    "    tools=tools,\n",
    "    llm=llm,\n",
    "    agent=AgentType.CHAT_CONVERSATIONAL_REACT_DESCRIPTION,  # MAIN REACT TYPE\n",
    "    memory=memory,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "\n",
    "#agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory)\n",
    "\n",
    "print(react_agent.invoke({\"input\": \"summarize langchain in 2 lines then tell me who created it\"}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba23354-1734-4506-99e3-ab039ab58d16",
   "metadata": {},
   "source": [
    "### STRUCTURED CHAT ZERO SHOT REACT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9cfb9ee-4c30-494f-972d-9e893d3826a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_4384\\1078109836.py:29: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = agent.run(\"Make a friendly title about langgraph tutorials\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m```json\n",
      "{\n",
      "  \"action\": \"TitleMaker\",\n",
      "  \"action_input\": {\n",
      "    \"topic\": \"langgraph tutorials\",\n",
      "    \"tone\": \"friendly\"\n",
      "  }\n",
      "}\n",
      "```\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mFriendly Title:langgraph tutorials in practice\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have a title that is friendly.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Final Answer\",\n",
      "  \"action_input\": \"Friendly Title:langgraph tutorials in practice\"\n",
      "}\n",
      "```\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "Result: Friendly Title:langgraph tutorials in practice\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.tools import StructuredTool\n",
    "\n",
    "class TitleInput(BaseModel):\n",
    "    topic: str\n",
    "    tone:str=\"concise\"\n",
    "\n",
    "def title_tool_fn(topic:str,tone:str=\"concise\") -> str:\n",
    "    return f\"{tone.title()} Title:{topic} in practice\"\n",
    "\n",
    "#Create the structured Tool\n",
    "title_tool = StructuredTool.from_function(\n",
    "    name = \"TitleMaker\",\n",
    "    func = title_tool_fn,\n",
    "    description = \"Generate a title given a topic and an optional tone\",\n",
    "    args_schema = TitleInput\n",
    ")\n",
    "\n",
    "#Initialize agent\n",
    "agent = initialize_agent(\n",
    "    tools = [title_tool],\n",
    "    llm = llm,\n",
    "    agent = AgentType.STRUCTURED_CHAT_ZERO_SHOT_REACT_DESCRIPTION,\n",
    "    verbose = True\n",
    ")\n",
    "\n",
    "# Run the agent\n",
    "\n",
    "result = agent.run(\"Make a friendly title about langgraph tutorials\")\n",
    "print(\"Result:\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69c52f7-5cfd-43c9-889c-a55ca1cd37f2",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "22298161-bec7-482e-864e-3fb36d01ee48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9aba251-540d-4b78-a1e6-6f6b64255d7d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tags=['FAISS', 'GoogleGenerativeAIEmbeddings'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x00000253FF688050> search_kwargs={}\n",
      "Answer:\n",
      " LangChain is a framework created by Harrison Chase for building applications with LLMs. It supports RAG, agents, memory, tools, and more, and is commonly used in chatbots, document Q&A, and AI workflows.\n"
     ]
    }
   ],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    text_data = f.read()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=100,\n",
    ")\n",
    "texts = splitter.split_text(text_data)\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")\n",
    "\n",
    "vectorstore = FAISS.from_texts(texts,embeddings)\n",
    "\n",
    "#vectorstore = Chroma.from_texts(texts, embedding=embeddings, persist_directory=\"./chroma_db\")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(retriever)\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\"     # simplest style: stuff retrieved chunks directly into prompt\n",
    ")\n",
    "\n",
    "query = \"Summarize what LangChain is and its core purpose.\"\n",
    "answer = qa_chain.invoke({\"query\": query})\n",
    "print(\"Answer:\\n\", answer[\"result\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74c3235a-a5ec-4997-bddf-3b6dc39fe0e7",
   "metadata": {},
   "source": [
    "### FileChatMessageHistory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1904059-a8d0-4bab-81f4-f4f22de36ec0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "History initialized, saving to: my_conversation_history.json\n",
      "---\n",
      "Total messages saved: 3\n",
      "[1] Human: Hello! My name is Alex.\n",
      "[2] Ai: Hello Alex! I am Gemini, a helpful AI assistant. How can I assist you today?\n",
      "[3] Human: What was my name again?\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from langchain_community.chat_message_histories import FileChatMessageHistory\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "FILE_PATH = \"my_conversation_history.json\"\n",
    "if os.path.exists(FILE_PATH):\n",
    "    # Optional: Delete the file to start clean for the demonstration\n",
    "    os.remove(FILE_PATH) \n",
    "\n",
    "# Initialize the history object\n",
    "chat_history = FileChatMessageHistory(file_path=FILE_PATH)\n",
    "\n",
    "print(f\"History initialized, saving to: {FILE_PATH}\")\n",
    "print(\"---\")\n",
    "\n",
    "# --- 2. Adding Messages ---\n",
    "\n",
    "# Add messages using convenience methods\n",
    "chat_history.add_user_message(\"Hello! My name is Alex.\")\n",
    "chat_history.add_ai_message(\"Hello Alex! I am Gemini, a helpful AI assistant. How can I assist you today?\")\n",
    "\n",
    "# Add a message using the BaseMessage object directly\n",
    "follow_up_user = HumanMessage(content=\"What was my name again?\")\n",
    "chat_history.add_message(follow_up_user)\n",
    "\n",
    "# --- 3. Retrieving and Displaying Messages ---\n",
    "messages = chat_history.messages \n",
    "\n",
    "print(f\"Total messages saved: {len(messages)}\")\n",
    "for i, message in enumerate(messages):\n",
    "    print(f\"[{i+1}] {message.type.capitalize()}: {message.content}\")\n",
    "\n",
    "print(\"---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1710f1-87bb-4d28-a39b-31d401520a67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
